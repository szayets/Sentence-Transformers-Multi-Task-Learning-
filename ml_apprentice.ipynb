{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NVFUr2_YCYZa"
      },
      "outputs": [],
      "source": [
        "# Importing the Required Libraries\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pre-trained model optimized for sentence embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # 'all-MiniLM-L6-v2' is a Lightweight BERT-based model optimized for sentence embeddings that is well-suited for similar tasks.\n",
        "\n",
        "\n",
        "# Example sentences for testing\n",
        "sentences = [\"Enter your coupon code at checkout to unlock exclusive discounts.\",\n",
        "             \"Use promo codes at checkout to enjoy extra savings on your favorite products.\",\n",
        "             \"Leave a review and share your experience with others.\"]\n",
        "\n",
        "# Define some custom categories for these sentences\n",
        "categories = {\n",
        "    \"Promotion\": 0,  # Label for promotion-related sentences\n",
        "    \"Customer Feedback\": 1,  # Label for customer feedback sentences\n",
        "}\n",
        "\n",
        "# Corresponding labels based on the custom categories\n",
        "labels = [categories[\"Promotion\"], categories[\"Promotion\"], categories[\"Customer Feedback\"]]\n",
        "\n",
        "# Generate embeddings for the sentences\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Print shape of the batch embeddings\n",
        "print(\"Embedding Shape:\", embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHegqS_XLVBK",
        "outputId": "0ea554f0-6ed7-4b02-d2e4-3a26b41122d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Shape: (3, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 3 sentences in our batch each represented by a 384-dimensional vector."
      ],
      "metadata": {
        "id": "kH-PZMuYQtep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences for testing\n",
        "sentences = [\"Your coupon code exclusive discounts is expired.\",\n",
        "             \"Use promo codes at checkout is not mandatory.\",\n",
        "             \"Your review is appreciated.\"]\n",
        "\n",
        "# Generate embeddings for the sentences\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "# Define the classification and sentiment heads (no class structure)\n",
        "classification_head = nn.Linear(384, 4)  # 384-dimensional input, 4 classes for sentence classification\n",
        "sentiment_head = nn.Linear(384, 3)  # 384-dimensional input, 3 classes for sentiment analysis\n",
        "\n",
        "# Apply Softmax to the outputs to get probabilities\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "# Get the classification and sentiment outputs\n",
        "classification_output = classification_head(embeddings)\n",
        "sentiment_output = sentiment_head(embeddings)\n",
        "\n",
        "# Apply Softmax to the outputs\n",
        "classification_probs = softmax(classification_output)\n",
        "sentiment_probs = softmax(sentiment_output)\n",
        "\n",
        "# Map the index of max probability to category\n",
        "classification_labels = ['Promotion Expiry', 'Coupon Usage', 'Customer Feedback', 'Discount Offers']\n",
        "sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    # Classification\n",
        "    classification_index = torch.argmax(classification_probs[i]).item()\n",
        "    classification_result = classification_labels[classification_index]\n",
        "\n",
        "    # Sentiment\n",
        "    sentiment_index = torch.argmax(sentiment_probs[i]).item()\n",
        "    sentiment_result = sentiment_labels[sentiment_index]\n",
        "\n",
        "    # Print out classification probabilities for debugging\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Classification Output (Category): {classification_result}\")\n",
        "    print(f\"Classification Probabilities: {classification_probs[i]}\")\n",
        "    print(f\"Sentiment Output: {sentiment_result}\")\n",
        "    print(f\"Sentiment Probabilities: {sentiment_probs[i]}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dD_79mDPcNO",
        "outputId": "68139474-d3a6-4ccb-c0b7-a3695565bf1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Your coupon code exclusive discounts is expired.\n",
            "Classification Output (Category): Promotion Expiry\n",
            "Classification Probabilities: tensor([0.2576, 0.2510, 0.2388, 0.2525], grad_fn=<SelectBackward0>)\n",
            "Sentiment Output: Positive\n",
            "Sentiment Probabilities: tensor([0.3346, 0.3276, 0.3378], grad_fn=<SelectBackward0>)\n",
            "\n",
            "\n",
            "Sentence: Use promo codes at checkout is not mandatory.\n",
            "Classification Output (Category): Coupon Usage\n",
            "Classification Probabilities: tensor([0.2543, 0.2551, 0.2367, 0.2540], grad_fn=<SelectBackward0>)\n",
            "Sentiment Output: Positive\n",
            "Sentiment Probabilities: tensor([0.3211, 0.3350, 0.3440], grad_fn=<SelectBackward0>)\n",
            "\n",
            "\n",
            "Sentence: Your review is appreciated.\n",
            "Classification Output (Category): Coupon Usage\n",
            "Classification Probabilities: tensor([0.2574, 0.2646, 0.2392, 0.2388], grad_fn=<SelectBackward0>)\n",
            "Sentiment Output: Neutral\n",
            "Sentiment Probabilities: tensor([0.3270, 0.3511, 0.3219], grad_fn=<SelectBackward0>)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Should the Entire Network be Frozen\n",
        "* Freezing the entire network is typically not ideal when working with task-specific heads that need to adapt to specific problems, such as sentiment analysis or sentence classification. However, it might be useful in cases where you’re limited by computational resources and want a quick solution.\n",
        "\n",
        "#### Should Only the Transformer Backbone be Frozen?\n",
        "* This approach allows the model to learn task-specific features while retaining the powerful language representations learned during pre-training.\n",
        "But if the transformer backbone is frozen, the model might not adapt perfectly to the nuances of the tasks.\n",
        "\n",
        "* Freezing one task-specific head means the model will adapt one task while keeping the other task’s head fixed. This can be useful in a multi-task learning scenario where one task is more complex or requires more adjustment.\n",
        "\n",
        "### Transfer learning process\n",
        "* In general, transfer learning is beneficial when you have a pre-trained model that has learned useful knowledge from a large dataset. By freezing the backbone and fine-tuning the heads, you can leverage the general language knowledge and apply it to specific tasks.\n"
      ],
      "metadata": {
        "id": "Dl-j1cgysbAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "# Loss functions and optimizer\n",
        "criterion_classification = nn.CrossEntropyLoss()\n",
        "criterion_sentiment = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(list(classification_head.parameters()) + list(sentiment_head.parameters()), lr=0.001)\n",
        "\n",
        "# Hypothetical Dataset (Sentences and Random Labels)\n",
        "sentences = [\"Enter your coupon code at checkout to unlock exclusive discounts.\",\n",
        "             \"Use promo codes at checkout to enjoy extra savings on your favorite products.\",\n",
        "             \"Leave a review and share your experience with others.\"]\n",
        "\n",
        "# Simulate sentence embeddings (Replace with real embeddings in actual training)\n",
        "embeddings = torch.randn(len(sentences), 384)\n",
        "\n",
        "# Random labels for classification and sentiment tasks\n",
        "classification_labels = torch.randint(0, 4, (len(sentences),))\n",
        "sentiment_labels = torch.randint(0, 3, (len(sentences),))\n",
        "\n",
        "# Training Loop (Single Epoch for Demonstration)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    classification_head.train()\n",
        "    sentiment_head.train()\n",
        "\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "    # Forward pass through both heads\n",
        "    classification_output = classification_head(embeddings)\n",
        "    sentiment_output = sentiment_head(embeddings)\n",
        "\n",
        "    # Compute Losses\n",
        "    loss_classification = criterion_classification(classification_output, classification_labels)\n",
        "    loss_sentiment = criterion_sentiment(sentiment_output, sentiment_labels)\n",
        "\n",
        "    total_loss = loss_classification + loss_sentiment  # Combined loss for multi-task learning\n",
        "\n",
        "    # Backward pass\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28WtDo2EuI47",
        "outputId": "fac1e696-7f73-4e6f-a0aa-f01046683596"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 2.6939\n",
            "Epoch [2/5], Loss: 2.2082\n",
            "Epoch [3/5], Loss: 1.7780\n",
            "Epoch [4/5], Loss: 1.4071\n",
            "Epoch [5/5], Loss: 1.0969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Loss is decreasing steadily across epochs, which is a good sign! It means the model is learning and adjusting its weights effectively."
      ],
      "metadata": {
        "id": "GLrAhMl7wNiu"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}